{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec4c804-799c-4aa3-80fe-5947dc5f2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTImageProcessor\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize\n",
    "\n",
    "from app.slow_classificator import ResClassifier, Classificator, VitClassifier\n",
    "\n",
    "dataset = CustomImageDataset(root_dir=\"/home/user1/hack/train_data_rkn/dataset\")\n",
    "# train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True,num_workers=4)\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.id2label = {k: v for k, v in enumerate(sorted(os.listdir(root_dir)))}\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.improcessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        \n",
    "        self.size = self.improcessor.size[\"height\"]\n",
    "        self.normalize = Normalize(\n",
    "            mean=self.improcessor.image_mean,\n",
    "            std=self.improcessor.image_std\n",
    "        )\n",
    "\n",
    "        self._transforms = Compose([\n",
    "            Resize((self.size, self.size)),\n",
    "            ToTensor(),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "        for cls in self.id2label.values():\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                for img_name in os.listdir(cls_folder):\n",
    "                    img_path = os.path.join(cls_folder, img_name)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(cls)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"pixel_values\": self.improcessor(\n",
    "                images=Image.open(self.image_paths[idx]).convert(\"RGB\")).pixel_values[0].squeeze(), # .squeeze()\n",
    "            \"labels\": self.label2id[self.labels[idx]]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19611685-16b2-475d-b640-8d61cd81696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08ba8e1b-dcc1-49c5-9f06-cc4a2153b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import tqdm\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import AveragePrecision\n",
    "from transformers import (\n",
    "    DefaultDataCollator,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    "    ViTForImageClassification,\n",
    "    ViTImageProcessor,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificatorONNX:\n",
    "    def __init__(self, model_path: str=\"vit_v4.onnx\", device=\"cpu\"):\n",
    "        self.device = device\n",
    "        hf_hub_download(repo_id=\"alan3333/hack_rkn_onnx\", filename=model_path, local_dir=\"./\")\n",
    "\n",
    "        self.session = ort.InferenceSession(model_path)\n",
    "        \n",
    "        self.image_processor = tv.transforms.Compose(\n",
    "            [\n",
    "                tv.transforms.Resize((224, 224)),\n",
    "                tv.transforms.ToTensor(),\n",
    "                tv.transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _transform(self, image: Image.Image) -> np.ndarray:\n",
    "        processed_image = self.image_processor(image).unsqueeze(0)\n",
    "        return processed_image.numpy().astype(np.float32)\n",
    "\n",
    "    def predict_proba_class(self, image: Image.Image) -> List[np.ndarray]:\n",
    "        processed_image = self._transform(image)\n",
    "    \n",
    "        outputs = self.session.run(None, {\"pixel_values\": processed_image})\n",
    "        probabilities = (np.exp(outputs)/np.sum(np.exp(outputs)))[0][0]\n",
    "        predicted_class = probabilities.argmax()\n",
    "       \n",
    "        return int(predicted_class), float(probabilities[predicted_class])\n",
    "\n",
    "    def predict(self, image: Image.Image) -> int:\n",
    "        return self.predict_proba_class(image)[0]\n",
    "\n",
    "    def predict_embedding(self, image: Image.Image) -> np.ndarray:\n",
    "        processed_image = self._transform(image)\n",
    "        outputs = self.session.run(None, {\"pixel_values\": processed_image})\n",
    "  \n",
    "        embedding = outputs[0][0]\n",
    "        return embedding.squeeze().tolist()\n",
    "\n",
    "    def predict_result(self, image: Image.Image) -> Dict[str, np.ndarray]:\n",
    "        predicted_class, probabilities = self.predict_proba_class(image)\n",
    "        embedding = self.predict_embedding(image)\n",
    "\n",
    "        return {\n",
    "            \"class\": predicted_class,\n",
    "            \"probs_class\": probabilities,\n",
    "            \"embedding\": embedding,\n",
    "        }\n",
    "\n",
    "\n",
    "classificator_instance = ClassificatorONNX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ac059-c697-4db2-87b5-00ab091153a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df7279f8-1b2e-480c-9980-898d04395f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassificatorONNX(\"vit_v4.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22cbf599-f224-445b-b4a4-11ea225ba005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 0.8637479543685913)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_proba_class(Image.open(dataset.image_paths[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84bdcc14-a176-42cb-b091-cf3d70b8bb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 6,\n",
       " 'probs_class': 0.8637479543685913,\n",
       " 'embedding': [-0.14666026830673218,\n",
       "  -0.02052750252187252,\n",
       "  -0.6322766542434692,\n",
       "  0.08765003085136414,\n",
       "  -0.12225610017776489,\n",
       "  -0.04616245627403259,\n",
       "  6.7528791427612305,\n",
       "  -0.1726534515619278,\n",
       "  2.1956520080566406,\n",
       "  0.17181754112243652,\n",
       "  -0.7886292338371277,\n",
       "  -0.28554877638816833,\n",
       "  -0.04729550704360008,\n",
       "  -0.9365891218185425,\n",
       "  -0.01777680218219757,\n",
       "  0.2869463562965393,\n",
       "  0.38678357005119324,\n",
       "  -0.13066191971302032,\n",
       "  0.6296558380126953,\n",
       "  -0.3088721036911011,\n",
       "  0.05625346302986145,\n",
       "  0.3677195906639099,\n",
       "  -0.722286581993103,\n",
       "  -0.1251567155122757,\n",
       "  0.4465736150741577,\n",
       "  1.6457087993621826,\n",
       "  -0.09712395071983337,\n",
       "  -0.700269877910614,\n",
       "  0.2704384922981262,\n",
       "  -0.2038898468017578,\n",
       "  -0.45533400774002075,\n",
       "  0.31055569648742676,\n",
       "  -0.8964690566062927,\n",
       "  -0.7936358451843262,\n",
       "  -0.14049625396728516,\n",
       "  -0.37208789587020874,\n",
       "  -0.6660160422325134,\n",
       "  0.3401380777359009,\n",
       "  -1.1864463090896606,\n",
       "  -0.13739338517189026,\n",
       "  2.5056159496307373,\n",
       "  -0.2826855480670929,\n",
       "  0.15811476111412048,\n",
       "  0.20631909370422363,\n",
       "  -0.17507439851760864,\n",
       "  -0.3684554696083069,\n",
       "  0.3973221480846405,\n",
       "  0.9004910588264465,\n",
       "  0.06009768694639206,\n",
       "  0.4267694354057312,\n",
       "  -1.3899658918380737,\n",
       "  0.1491352915763855,\n",
       "  0.4781431257724762,\n",
       "  -0.8745383024215698,\n",
       "  -0.003258451819419861,\n",
       "  -0.07052920013666153,\n",
       "  0.949805736541748,\n",
       "  -1.1020869016647339,\n",
       "  -0.31113213300704956,\n",
       "  0.37987127900123596,\n",
       "  -0.3020508289337158,\n",
       "  -0.0046980977058410645,\n",
       "  1.9477821588516235,\n",
       "  -0.032264649868011475,\n",
       "  0.3744490146636963,\n",
       "  0.002100050449371338,\n",
       "  0.34871506690979004,\n",
       "  0.7695929408073425,\n",
       "  -0.16719987988471985,\n",
       "  -0.2338760644197464,\n",
       "  -0.6409879922866821,\n",
       "  -1.1314952373504639,\n",
       "  -0.3406287133693695,\n",
       "  0.90190589427948,\n",
       "  0.5459424257278442,\n",
       "  -0.03644642233848572,\n",
       "  -0.02349250018596649,\n",
       "  0.6400489807128906,\n",
       "  0.5102700591087341,\n",
       "  -0.30364617705345154,\n",
       "  0.15531012415885925,\n",
       "  -0.4324553608894348,\n",
       "  0.024030029773712158,\n",
       "  -0.897279679775238,\n",
       "  -0.3606501817703247,\n",
       "  0.9853051900863647,\n",
       "  -1.1396944522857666,\n",
       "  -0.44437214732170105,\n",
       "  -0.1628762036561966,\n",
       "  0.051814302802085876,\n",
       "  0.0988580584526062,\n",
       "  0.337689071893692,\n",
       "  -0.2853474020957947,\n",
       "  -0.5501799583435059,\n",
       "  -0.6394882798194885,\n",
       "  0.07928859442472458,\n",
       "  -0.6103098392486572,\n",
       "  0.15850472450256348,\n",
       "  -0.7630579471588135,\n",
       "  0.2677139639854431,\n",
       "  0.06414026767015457,\n",
       "  -1.0436066389083862,\n",
       "  -0.7161902189254761,\n",
       "  -0.05698262155056,\n",
       "  0.7446850538253784,\n",
       "  -0.5350902080535889]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_result(Image.open(dataset.image_paths[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d5235-0c59-422d-a51c-1e11c639c476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c01b8a16-3d3a-48eb-9de8-0aa5b9b0c51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0926b1cd76d54beea1f326eb0bb46666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'README.md'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(repo_id=\"alan3333/hack_rkn_onnx\", filename=\"README.md\", local_dir=\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52a03e58-4fcd-4e0f-8055-950170436b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0\": \".DS_Store\", \"1\": \"Accordion\", \"2\": \"Adhesive tape\", \"3\": \"Aircraft\", \"4\": \"Airplane\", \"5\": \"Alarm clock\", \"6\": \"Alpaca\", \"7\": \"Ambulance\", \"8\": \"Animal\", \"9\": \"Ant\", \"10\": \"Apple\", \"11\": \"Artichoke\", \"12\": \"Banana\", \"13\": \"Barge\", \"14\": \"Bathtub\", \"15\": \"Belt\", \"16\": \"Binoculars\", \"17\": \"Bottle\", \"18\": \"Bow and arrow\", \"19\": \"Bread\", \"20\": \"Briefcase\", \"21\": \"Broccoli\", \"22\": \"Camera\", \"23\": \"Cannon\", \"24\": \"Cassette deck\", \"25\": \"Cat\", \"26\": \"Cello\", \"27\": \"Christmas tree\", \"28\": \"Coin\", \"29\": \"Common fig\", \"30\": \"Cosmetics\", \"31\": \"Cucumber\", \"32\": \"Cutting board\", \"33\": \"Earrings\", \"34\": \"Elephant\", \"35\": \"Fedora\", \"36\": \"Flashlight\", \"37\": \"Frying pan\", \"38\": \"Glasses\", \"39\": \"Glove\", \"40\": \"Goat\", \"41\": \"Goldfish\", \"42\": \"Grape\", \"43\": \"Harp\", \"44\": \"Hat\", \"45\": \"Helmet\", \"46\": \"High heels\", \"47\": \"Hippopotamus\", \"48\": \"Honeycomb\", \"49\": \"Horse\", \"50\": \"Insect\", \"51\": \"Invertebrate\", \"52\": \"Ipod\", \"53\": \"Isopod\", \"54\": \"Jacket\", \"55\": \"Jet ski\", \"56\": \"Koala\", \"57\": \"Land vehicle\", \"58\": \"Laptop\", \"59\": \"Lighthouse\", \"60\": \"Lily\", \"61\": \"Limousine\", \"62\": \"Lion\", \"63\": \"Lipstick\", \"64\": \"Magpie\", \"65\": \"Microwave oven\", \"66\": \"Mixer\", \"67\": \"Monkey\", \"68\": \"Pancake\", \"69\": \"Parking meter\", \"70\": \"Pear\", \"71\": \"Piano\", \"72\": \"Plastic bag\", \"73\": \"Rabbit\", \"74\": \"Reptile\", \"75\": \"Rhinoceros\", \"76\": \"Rocket\", \"77\": \"Scarf\", \"78\": \"Seahorse\", \"79\": \"Shelf\", \"80\": \"Skull\", \"81\": \"Snowboard\", \"82\": \"Sombrero\", \"83\": \"Sparrow\", \"84\": \"Spatula\", \"85\": \"Spoon\", \"86\": \"Submarine\", \"87\": \"Tablet computer\", \"88\": \"Tap\", \"89\": \"Tea\", \"90\": \"Toilet paper\", \"91\": \"Vase\", \"92\": \"Wheel\", \"93\": \"Whisk\", \"94\": \"Whiteboard\", \"95\": \"Willow\", \"96\": \"Wine rack\", \"97\": \"Winter melon\", \"98\": \"Wok\", \"99\": \"Woman\", \"100\": \"Wood-burning stove\", \"101\": \"Woodpecker\", \"102\": \"Worm\", \"103\": \"Wrench\", \"104\": \"Zebra\", \"105\": \"Zucchini\"}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json \n",
    "json.dumps({k: v for k, v in enumerate(sorted(os.listdir(\"/home/user1/hack/train_data_rkn/dataset\")))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22c7fa-56d6-466a-b77f-e6c3fcc3bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

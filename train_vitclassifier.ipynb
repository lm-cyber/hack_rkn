{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0215a5ba-5723-4f1f-a0c3-cf6ff9261828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27cde17-5263-48cc-83a1-14105fd4a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815f88d7-8e55-455b-8111-38a5d18dfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.slow_classificator import ResClassifier, Classificator, VitClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf708d-a9c9-438a-b2bf-dd1537a08e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet34, ResNet, resnext101_64x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f172320b-5baa-4739-8baf-ece98bab82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.id2label = {k: v for k, v in enumerate(sorted(os.listdir(root_dir)))}\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.improcessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        \n",
    "        self.size = self.improcessor.size[\"height\"]\n",
    "        self.normalize = Normalize(\n",
    "            mean=self.improcessor.image_mean,\n",
    "            std=self.improcessor.image_std\n",
    "        )\n",
    "\n",
    "        self._transforms = Compose([\n",
    "            Resize((self.size, self.size)),\n",
    "            ToTensor(),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "        for cls in self.id2label.values():\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                for img_name in os.listdir(cls_folder):\n",
    "                    img_path = os.path.join(cls_folder, img_name)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(cls)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"pixel_values\": self.improcessor(\n",
    "                images=Image.open(self.image_paths[idx]).convert(\"RGB\")).pixel_values[0].squeeze(), # .squeeze()\n",
    "            \"labels\": self.label2id[self.labels[idx]]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1bd84-08be-4c45-8414-92e2e336ff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20bbc14-6a17-4bd9-817c-11f5ed84a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(root_dir=\"/home/user1/hack/train_data_rkn/dataset\")\n",
    "# train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fdd6aba-7b29-44b1-a835-53a292f39a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': array([[[-0.69411767, -0.7019608 , -0.6784314 , ...,  0.21568632,\n",
       "          -0.12941176, -0.27843136],\n",
       "         [-0.7254902 , -0.7254902 , -0.7176471 , ...,  0.0196079 ,\n",
       "          -0.03529412, -0.1372549 ],\n",
       "         [-0.7411765 , -0.7647059 , -0.77254903, ..., -0.17647058,\n",
       "          -0.04313725, -0.02745098],\n",
       "         ...,\n",
       "         [-0.69411767, -0.84313726, -0.94509804, ..., -0.90588236,\n",
       "          -0.8980392 , -0.8980392 ],\n",
       "         [-0.69411767, -0.78039217, -0.92156863, ..., -0.92941177,\n",
       "          -0.9372549 , -0.92941177],\n",
       "         [-0.78039217, -0.79607844, -0.9137255 , ..., -0.9137255 ,\n",
       "          -0.92156863, -0.92156863]],\n",
       " \n",
       "        [[-0.60784316, -0.60784316, -0.5921569 , ...,  0.30196083,\n",
       "          -0.03529412, -0.12941176],\n",
       "         [-0.64705884, -0.64705884, -0.64705884, ...,  0.13725495,\n",
       "           0.09019613,  0.01176476],\n",
       "         [-0.6627451 , -0.69411767, -0.7019608 , ..., -0.03529412,\n",
       "           0.12156868,  0.12156868],\n",
       "         ...,\n",
       "         [-0.6784314 , -0.827451  , -0.94509804, ..., -0.8980392 ,\n",
       "          -0.8980392 , -0.8980392 ],\n",
       "         [-0.6862745 , -0.77254903, -0.9137255 , ..., -0.92156863,\n",
       "          -0.92941177, -0.92941177],\n",
       "         [-0.77254903, -0.79607844, -0.9137255 , ..., -0.90588236,\n",
       "          -0.9137255 , -0.9137255 ]],\n",
       " \n",
       "        [[-0.9607843 , -0.92941177, -0.8745098 , ...,  0.02745104,\n",
       "          -0.36470586, -0.6       ],\n",
       "         [-0.94509804, -0.92156863, -0.8901961 , ..., -0.18431371,\n",
       "          -0.23921567, -0.44313723],\n",
       "         [-0.9529412 , -0.9372549 , -0.92156863, ..., -0.4352941 ,\n",
       "          -0.27058822, -0.26274508],\n",
       "         ...,\n",
       "         [-0.654902  , -0.81960785, -0.9372549 , ..., -0.88235295,\n",
       "          -0.88235295, -0.88235295],\n",
       "         [-0.654902  , -0.75686276, -0.9137255 , ..., -0.90588236,\n",
       "          -0.9137255 , -0.92156863],\n",
       "         [-0.7490196 , -0.77254903, -0.90588236, ..., -0.8901961 ,\n",
       "          -0.8980392 , -0.90588236]]], dtype=float32),\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a0b4e6-ba68-47fc-bad3-d751ab3d872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([106]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([106, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = VitClassifier(id2label=dataset.id2label, label2id=dataset.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d74b741-6391-40cd-ba13-6a895ba9eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 1:27:12, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.654415</td>\n",
       "      <td>0.344189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.694040</td>\n",
       "      <td>0.608644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.978977</td>\n",
       "      <td>0.717972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.538348</td>\n",
       "      <td>0.769273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.274908</td>\n",
       "      <td>0.795820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.111773</td>\n",
       "      <td>0.811154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.011399</td>\n",
       "      <td>0.819799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946715</td>\n",
       "      <td>0.825168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.740100</td>\n",
       "      <td>0.908505</td>\n",
       "      <td>0.827045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.740100</td>\n",
       "      <td>0.886341</td>\n",
       "      <td>0.828190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.740100</td>\n",
       "      <td>0.874839</td>\n",
       "      <td>0.828598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.740100</td>\n",
       "      <td>0.871186</td>\n",
       "      <td>0.828912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8.19672131147541 - Step 500 - Loss: 1.7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "classifier.tune(dataset,\n",
    "                device=\"cuda\",\n",
    "                epochs=12,\n",
    "                batch_size=256,\n",
    "                lr=2e-5, test_split=True,\n",
    "                output_dir=\"./vit_v5_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c8f22-e501-4b8d-b237-939da64f12c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10f6cc9-2774-45b2-ae0b-c1c85fa452f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
       "  \"architectures\": [\n",
       "    \"ViTForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \".DS_Store\",\n",
       "    \"1\": \"Accordion\",\n",
       "    \"2\": \"Adhesive tape\",\n",
       "    \"3\": \"Aircraft\",\n",
       "    \"4\": \"Airplane\",\n",
       "    \"5\": \"Alarm clock\",\n",
       "    \"6\": \"Alpaca\",\n",
       "    \"7\": \"Ambulance\",\n",
       "    \"8\": \"Animal\",\n",
       "    \"9\": \"Ant\",\n",
       "    \"10\": \"Apple\",\n",
       "    \"11\": \"Artichoke\",\n",
       "    \"12\": \"Banana\",\n",
       "    \"13\": \"Barge\",\n",
       "    \"14\": \"Bathtub\",\n",
       "    \"15\": \"Belt\",\n",
       "    \"16\": \"Binoculars\",\n",
       "    \"17\": \"Bottle\",\n",
       "    \"18\": \"Bow and arrow\",\n",
       "    \"19\": \"Bread\",\n",
       "    \"20\": \"Briefcase\",\n",
       "    \"21\": \"Broccoli\",\n",
       "    \"22\": \"Camera\",\n",
       "    \"23\": \"Cannon\",\n",
       "    \"24\": \"Cassette deck\",\n",
       "    \"25\": \"Cat\",\n",
       "    \"26\": \"Cello\",\n",
       "    \"27\": \"Christmas tree\",\n",
       "    \"28\": \"Coin\",\n",
       "    \"29\": \"Common fig\",\n",
       "    \"30\": \"Cosmetics\",\n",
       "    \"31\": \"Cucumber\",\n",
       "    \"32\": \"Cutting board\",\n",
       "    \"33\": \"Earrings\",\n",
       "    \"34\": \"Elephant\",\n",
       "    \"35\": \"Fedora\",\n",
       "    \"36\": \"Flashlight\",\n",
       "    \"37\": \"Frying pan\",\n",
       "    \"38\": \"Glasses\",\n",
       "    \"39\": \"Glove\",\n",
       "    \"40\": \"Goat\",\n",
       "    \"41\": \"Goldfish\",\n",
       "    \"42\": \"Grape\",\n",
       "    \"43\": \"Harp\",\n",
       "    \"44\": \"Hat\",\n",
       "    \"45\": \"Helmet\",\n",
       "    \"46\": \"High heels\",\n",
       "    \"47\": \"Hippopotamus\",\n",
       "    \"48\": \"Honeycomb\",\n",
       "    \"49\": \"Horse\",\n",
       "    \"50\": \"Insect\",\n",
       "    \"51\": \"Invertebrate\",\n",
       "    \"52\": \"Ipod\",\n",
       "    \"53\": \"Isopod\",\n",
       "    \"54\": \"Jacket\",\n",
       "    \"55\": \"Jet ski\",\n",
       "    \"56\": \"Koala\",\n",
       "    \"57\": \"Land vehicle\",\n",
       "    \"58\": \"Laptop\",\n",
       "    \"59\": \"Lighthouse\",\n",
       "    \"60\": \"Lily\",\n",
       "    \"61\": \"Limousine\",\n",
       "    \"62\": \"Lion\",\n",
       "    \"63\": \"Lipstick\",\n",
       "    \"64\": \"Magpie\",\n",
       "    \"65\": \"Microwave oven\",\n",
       "    \"66\": \"Mixer\",\n",
       "    \"67\": \"Monkey\",\n",
       "    \"68\": \"Pancake\",\n",
       "    \"69\": \"Parking meter\",\n",
       "    \"70\": \"Pear\",\n",
       "    \"71\": \"Piano\",\n",
       "    \"72\": \"Plastic bag\",\n",
       "    \"73\": \"Rabbit\",\n",
       "    \"74\": \"Reptile\",\n",
       "    \"75\": \"Rhinoceros\",\n",
       "    \"76\": \"Rocket\",\n",
       "    \"77\": \"Scarf\",\n",
       "    \"78\": \"Seahorse\",\n",
       "    \"79\": \"Shelf\",\n",
       "    \"80\": \"Skull\",\n",
       "    \"81\": \"Snowboard\",\n",
       "    \"82\": \"Sombrero\",\n",
       "    \"83\": \"Sparrow\",\n",
       "    \"84\": \"Spatula\",\n",
       "    \"85\": \"Spoon\",\n",
       "    \"86\": \"Submarine\",\n",
       "    \"87\": \"Tablet computer\",\n",
       "    \"88\": \"Tap\",\n",
       "    \"89\": \"Tea\",\n",
       "    \"90\": \"Toilet paper\",\n",
       "    \"91\": \"Vase\",\n",
       "    \"92\": \"Wheel\",\n",
       "    \"93\": \"Whisk\",\n",
       "    \"94\": \"Whiteboard\",\n",
       "    \"95\": \"Willow\",\n",
       "    \"96\": \"Wine rack\",\n",
       "    \"97\": \"Winter melon\",\n",
       "    \"98\": \"Wok\",\n",
       "    \"99\": \"Woman\",\n",
       "    \"100\": \"Wood-burning stove\",\n",
       "    \"101\": \"Woodpecker\",\n",
       "    \"102\": \"Worm\",\n",
       "    \"103\": \"Wrench\",\n",
       "    \"104\": \"Zebra\",\n",
       "    \"105\": \"Zucchini\"\n",
       "  },\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \".DS_Store\": 0,\n",
       "    \"Accordion\": 1,\n",
       "    \"Adhesive tape\": 2,\n",
       "    \"Aircraft\": 3,\n",
       "    \"Airplane\": 4,\n",
       "    \"Alarm clock\": 5,\n",
       "    \"Alpaca\": 6,\n",
       "    \"Ambulance\": 7,\n",
       "    \"Animal\": 8,\n",
       "    \"Ant\": 9,\n",
       "    \"Apple\": 10,\n",
       "    \"Artichoke\": 11,\n",
       "    \"Banana\": 12,\n",
       "    \"Barge\": 13,\n",
       "    \"Bathtub\": 14,\n",
       "    \"Belt\": 15,\n",
       "    \"Binoculars\": 16,\n",
       "    \"Bottle\": 17,\n",
       "    \"Bow and arrow\": 18,\n",
       "    \"Bread\": 19,\n",
       "    \"Briefcase\": 20,\n",
       "    \"Broccoli\": 21,\n",
       "    \"Camera\": 22,\n",
       "    \"Cannon\": 23,\n",
       "    \"Cassette deck\": 24,\n",
       "    \"Cat\": 25,\n",
       "    \"Cello\": 26,\n",
       "    \"Christmas tree\": 27,\n",
       "    \"Coin\": 28,\n",
       "    \"Common fig\": 29,\n",
       "    \"Cosmetics\": 30,\n",
       "    \"Cucumber\": 31,\n",
       "    \"Cutting board\": 32,\n",
       "    \"Earrings\": 33,\n",
       "    \"Elephant\": 34,\n",
       "    \"Fedora\": 35,\n",
       "    \"Flashlight\": 36,\n",
       "    \"Frying pan\": 37,\n",
       "    \"Glasses\": 38,\n",
       "    \"Glove\": 39,\n",
       "    \"Goat\": 40,\n",
       "    \"Goldfish\": 41,\n",
       "    \"Grape\": 42,\n",
       "    \"Harp\": 43,\n",
       "    \"Hat\": 44,\n",
       "    \"Helmet\": 45,\n",
       "    \"High heels\": 46,\n",
       "    \"Hippopotamus\": 47,\n",
       "    \"Honeycomb\": 48,\n",
       "    \"Horse\": 49,\n",
       "    \"Insect\": 50,\n",
       "    \"Invertebrate\": 51,\n",
       "    \"Ipod\": 52,\n",
       "    \"Isopod\": 53,\n",
       "    \"Jacket\": 54,\n",
       "    \"Jet ski\": 55,\n",
       "    \"Koala\": 56,\n",
       "    \"Land vehicle\": 57,\n",
       "    \"Laptop\": 58,\n",
       "    \"Lighthouse\": 59,\n",
       "    \"Lily\": 60,\n",
       "    \"Limousine\": 61,\n",
       "    \"Lion\": 62,\n",
       "    \"Lipstick\": 63,\n",
       "    \"Magpie\": 64,\n",
       "    \"Microwave oven\": 65,\n",
       "    \"Mixer\": 66,\n",
       "    \"Monkey\": 67,\n",
       "    \"Pancake\": 68,\n",
       "    \"Parking meter\": 69,\n",
       "    \"Pear\": 70,\n",
       "    \"Piano\": 71,\n",
       "    \"Plastic bag\": 72,\n",
       "    \"Rabbit\": 73,\n",
       "    \"Reptile\": 74,\n",
       "    \"Rhinoceros\": 75,\n",
       "    \"Rocket\": 76,\n",
       "    \"Scarf\": 77,\n",
       "    \"Seahorse\": 78,\n",
       "    \"Shelf\": 79,\n",
       "    \"Skull\": 80,\n",
       "    \"Snowboard\": 81,\n",
       "    \"Sombrero\": 82,\n",
       "    \"Sparrow\": 83,\n",
       "    \"Spatula\": 84,\n",
       "    \"Spoon\": 85,\n",
       "    \"Submarine\": 86,\n",
       "    \"Tablet computer\": 87,\n",
       "    \"Tap\": 88,\n",
       "    \"Tea\": 89,\n",
       "    \"Toilet paper\": 90,\n",
       "    \"Vase\": 91,\n",
       "    \"Wheel\": 92,\n",
       "    \"Whisk\": 93,\n",
       "    \"Whiteboard\": 94,\n",
       "    \"Willow\": 95,\n",
       "    \"Wine rack\": 96,\n",
       "    \"Winter melon\": 97,\n",
       "    \"Wok\": 98,\n",
       "    \"Woman\": 99,\n",
       "    \"Wood-burning stove\": 100,\n",
       "    \"Woodpecker\": 101,\n",
       "    \"Worm\": 102,\n",
       "    \"Wrench\": 103,\n",
       "    \"Zebra\": 104,\n",
       "    \"Zucchini\": 105\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.2\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c475b516-41b7-454c-94db-c73ee491099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at /home/user1/solve/vit_overfit_last_results/checkpoint-684 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:172: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:178: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to vit_v4.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "def load_and_save_model_to_onnx(output_path, dataset, device=\"cpu\"):\n",
    "    input_shape = np.expand_dims(dataset[0][\"pixel_values\"], axis=0).shape\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model, # model = AutoModel.from_pretrain(checkpoint)\n",
    "        torch.randn(*input_shape).to(device),\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"pixel_values\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\"pixel_values\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
    "    )\n",
    "\n",
    "    print(f\"Model successfully saved to {output_path}\")\n",
    "\n",
    "load_and_save_model_to_onnx(\"/home/user1/solve/vit_overfit_last_results/checkpoint-684\", \"vit_v4.onnx\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d7add-a7ea-4d4c-b2d1-100e1c77f10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
